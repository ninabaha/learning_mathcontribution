{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "支持向量机SVM是从线性可分情况下的最优分类面提出的。所谓最优分类，就是要求分类线不但能够将两类无错误的分开，而且两类之间的分类间隔最大。推广到高维空间，最优分类线就成为最优分类面。\n",
    "\n",
    "考虑这样一个问题：在一个n维欧式空间内有红蓝两组点，怎么找到一个超平面将它们分隔开？为了方便大家理解，考虑最简单的情况，平面直角坐标系内有红蓝两组点，用一条直线将它们分隔开，如图所示：\n",
    "\n",
    "\n",
    "这似乎是很容易的一件事，直线的斜率可以不同，斜率相同的情况下截距也可以不同，有很多种分法都可以把两组样本点分割开来。但是，哪一种分法是最好的呢？这里首先就需要定义怎样才能算最好。不妨称：正样本到直线的距离与负样本到直线距离之和最大就是最好。样本到直线的距离定义为样本中所有点到直线距离的最小值。事实上，如果同学们还记得高中解析几何的知识，就知道点到直线的距离等于过这一点作直线平行线进而求解两平行线之间的距离。\n",
    "\n",
    "所以，目标函数也就是求这样一条直线：过正样本和负样本中各自离直线最近的两点作平行线，使两平行线之间距离最大。为了表示样本的正负性，不妨记正样本标签为+1，负样本标签为-1，可以得到直线的数学表达形式："
   ],
   "id": "accf6d6808a73966"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这个时候我们就发现了问题：不是所有的样本都可以用一条直线粗暴地分割开的。即使用直线去分割，一定会有一些样本点会被分错。如何让支持向量机具备容错性呢？这时，就需要借助软间隔的概念。你可以理解为，软间隔是一种正则化手段，它允许支持向量机有一个较小的误差率。加入一些松弛变量模型就变成了：",
   "id": "37ea8fc572ebed26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "很显然，经过映射以后，样本点用一个平面就可以完全把它们分开。那么这样的一个映射函数本质上是一个升维操作，这个函数也叫核函数。核方法使得数据能够由低维映射向高维，让形如异或问题这样的线性不可分问题得到解决。常用的核函数包括表8.2中列出的几种。",
   "id": "abdab48f135ad3b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm import tqdm, trange\n",
    "data = np.loadtxt('linear.csv', delimiter=',')\n",
    "print('数据集大小：', len(data))\n",
    "x = data[:, :2]\n",
    "y = data[:, 2]\n",
    "# 数据集可视化\n",
    "plt.figure()\n",
    "plt.scatter(x[y == -1, 0], x[y == -1, 1], color='red', label='y=-1')\n",
    "plt.scatter(x[y == 1, 0], x[y == 1, 1], color='blue', marker='x', label='y=1')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "c03d0eada8b5cbc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def SMO(x, y, ker, C, max_iter):\n",
    "\n",
    "    '''\n",
    "\n",
    "    SMO算法\n",
    "\n",
    "    x，y：样本的值和类别\n",
    "\n",
    "    ker：核函数，与线性回归中核函数的含义相同\n",
    "\n",
    "    C：惩罚系数\n",
    "\n",
    "    max_iter：最大迭代次数\n",
    "\n",
    "    '''\n",
    "\n",
    "    # 初始化参数\n",
    "    m = x.shape[0]\n",
    "    alpha = np.zeros(m)\n",
    "    # 预先计算所有向量的两两内积，减少重复计算\n",
    "    K = np.zeros((m, m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            K[i, j] = ker(x[i], x[j])\n",
    "    for l in trange(max_iter):\n",
    "        # 开始迭代\n",
    "        for i in range(m):\n",
    "            # 有m个参数，每一轮迭代中依次更新\n",
    "            # 固定参数alpha_i与另一个随机参数alpha_j，并且保证i与j不相等\n",
    "            j = np.random.choice([l for l in range(m) if l != i])\n",
    "            # 用-b/2a更新alpha_i的值\n",
    "            eta = K[j, j] + K[i, i] - 2 * K[i, j] # 分母\n",
    "            e_i = np.sum(y * alpha * K[:, i]) - y[i] # 分子\n",
    "            e_j = np.sum(y * alpha * K[:, j]) - y[j]\n",
    "            alpha_i = alpha[i] + y[i] * (e_j - e_i) / (eta + 1e-5) # 防止除以0\n",
    "            zeta = alpha[i] * y[i] + alpha[j] * y[j]\n",
    "            # 将alpha_i和对应的alpha_j保持在[0,C]区间\n",
    "            # 0 <= (zeta - y_j * alpha_j) / y_i <= C\n",
    "            if y[i] == y[j]:\n",
    "                lower = max(0, zeta / y[i] - C)\n",
    "\n",
    "                upper = min(C, zeta / y[i])\n",
    "\n",
    "            else:\n",
    "\n",
    "                lower = max(0, zeta / y[i])\n",
    "\n",
    "                upper = min(C, zeta / y[i] + C)\n",
    "\n",
    "            alpha_i = np.clip(alpha_i, lower, upper)\n",
    "\n",
    "            alpha_j = (zeta - y[i] * alpha_i) / y[j]\n",
    "            # 更新参数\n",
    "            alpha[i], alpha[j] = alpha_i, alpha_j\n",
    "    return alpha\n",
    "# 设置超参数\n",
    "C = 1e8 # 由于数据集完全线性可分，我们不引入松弛变量\n",
    "max_iter = 1000\n",
    "np.random.seed(0)\n",
    "alpha = SMO(x, y, ker=np.inner, C=C, max_iter=max_iter)\n",
    "# 用alpha计算w，b和支持向量\n",
    "sup_idx = alpha > 1e-5 # 支持向量的系数不为零\n",
    "print('支持向量个数：', np.sum(sup_idx))\n",
    "w = np.sum((alpha[sup_idx] * y[sup_idx]).reshape(-1, 1) * x[sup_idx], axis=0)\n",
    "wx = x @ w.reshape(-1, 1)\n",
    "b = -0.5 * (np.max(wx[y == -1]) + np.min(wx[y == 1]))\n",
    "print('参数：', w, b)\n",
    "# 绘图\n",
    "X = np.linspace(np.min(x[:, 0]), np.max(x[:, 0]), 100)\n",
    "Y = -(w[0] * X + b) / (w[1] + 1e-5)\n",
    "plt.figure()\n",
    "plt.scatter(x[y == -1, 0], x[y == -1, 1], color='red', label='y=-1')\n",
    "plt.scatter(x[y == 1, 0], x[y == 1, 1], marker='x', color='blue', label='y=1')\n",
    "plt.plot(X, Y, color='black')\n",
    "# 用圆圈标记出支持向量\n",
    "plt.scatter(x[sup_idx, 0], x[sup_idx, 1], marker='o', color='none', \n",
    "    edgecolor='purple', s=150, label='support vectors')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "c70a3c629dcfe42c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "一个线性不可分的例子",
   "id": "dd01e85271cbde96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = np.loadtxt('spiral.csv', delimiter=',')\n",
    "\n",
    "print('数据集大小：', len(data))\n",
    "\n",
    "x = data[:, :2]\n",
    "\n",
    "y = data[:, 2]\n",
    "\n",
    "# 数据集可视化\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(x[y == -1, 0], x[y == -1, 1], color='red', label='y=-1')\n",
    "\n",
    "plt.scatter(x[y == 1, 0], x[y == 1, 1], marker='x', color='blue', label='y=1')\n",
    "\n",
    "plt.xlabel(r'$x_1$')\n",
    "\n",
    "plt.ylabel(r'$x_2$')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.axis('square')\n",
    "\n",
    "plt.show()\n",
    "# 简单多项式核\n",
    "\n",
    "def simple_poly_kernel(d):\n",
    "\n",
    "    def k(x, y): \n",
    "\n",
    "        return np.inner(x, y) ** d\n",
    "\n",
    "    return k\n",
    "\n",
    "# RBF核\n",
    "\n",
    "def rbf_kernel(sigma):\n",
    "\n",
    "    def k(x, y):\n",
    "\n",
    "        return np.exp(-np.inner(x - y, x - y) / (2.0 * sigma ** 2))\n",
    "\n",
    "    return k\n",
    "\n",
    "# 余弦相似度核\n",
    "\n",
    "def cos_kernel(x, y):\n",
    "\n",
    "    return np.inner(x, y) / np.linalg.norm(x, 2) / np.linalg.norm(y, 2)\n",
    "\n",
    "# sigmoid核\n",
    "\n",
    "def sigmoid_kernel(beta, c):\n",
    "\n",
    "    def k(x, y):\n",
    "\n",
    "        return np.tanh(beta * np.inner(x, y) + c)\n",
    "\n",
    "    return k\n",
    "\n",
    "#测试不同核函数下样本会被如何核化：\n",
    "\n",
    "kernels = [\n",
    "\n",
    "    simple_poly_kernel(3), \n",
    "\n",
    "    rbf_kernel(0.1), \n",
    "\n",
    "    cos_kernel, \n",
    "\n",
    "    sigmoid_kernel(1, -1)\n",
    "\n",
    "]\n",
    "\n",
    "ker_names = ['Poly(3)', 'RBF(0.1)', 'Cos', 'Sigmoid(1,-1)']\n",
    "\n",
    "C = 1e8\n",
    "\n",
    "max_iter = 500\n",
    "\n",
    "# 绘图准备，构造网格\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "cmap = ListedColormap(['coral', 'royalblue'])\n",
    "\n",
    "# 开始求解 SVM\n",
    "\n",
    "for i in range(len(kernels)):\n",
    "\n",
    "    print('核函数：', ker_names[i])\n",
    "\n",
    "    alpha = SMO(x, y, kernels[i], C=C, max_iter=max_iter)\n",
    "\n",
    "    sup_idx = alpha > 1e-5 # 支持向量的系数不为零\n",
    "\n",
    "    sup_x = x[sup_idx] # 支持向量\n",
    "\n",
    "    sup_y = y[sup_idx]\n",
    "\n",
    "    sup_alpha = alpha[sup_idx]\n",
    "\n",
    "    # 用支持向量计算 w^T*x\n",
    "\n",
    "    def wx(x_new):\n",
    "\n",
    "        s = 0\n",
    "\n",
    "        for xi, yi, ai in zip(sup_x, sup_y, sup_alpha):\n",
    "\n",
    "            s += yi * ai * kernels[i](xi, x_new)\n",
    "\n",
    "        return s\n",
    "\n",
    "    # 计算b*\n",
    "\n",
    "    neg = [wx(xi) for xi in sup_x[sup_y == -1]]\n",
    "\n",
    "    pos = [wx(xi) for xi in sup_x[sup_y == 1]]\n",
    "\n",
    "    b = -0.5 * (np.max(neg) + np.min(pos))\n",
    "\n",
    "    # 构造网格并用 SVM 预测分类\n",
    "\n",
    "    G = np.linspace(-1.5, 1.5, 100)\n",
    "\n",
    "    G = np.meshgrid(G, G)\n",
    "\n",
    "    X = np.array([G[0].flatten(), G[1].flatten()]).T # 转换为每行一个向量的形式\n",
    "\n",
    "    Y = np.array([wx(xi) + b for xi in X])\n",
    "\n",
    "    Y[Y < 0] = -1\n",
    "\n",
    "    Y[Y >= 0] = 1\n",
    "\n",
    "    Y = Y.reshape(G[0].shape)\n",
    "\n",
    "    axs[i].contourf(G[0], G[1], Y, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # 绘制原数据集的点\n",
    "\n",
    "    axs[i].scatter(x[y == -1, 0], x[y == -1, 1], color='red', label='y=-1')\n",
    "\n",
    "    axs[i].scatter(x[y == 1, 0], x[y == 1, 1], marker='x', color='blue', label='y=1')\n",
    "\n",
    "    axs[i].set_title(ker_names[i])\n",
    "\n",
    "    axs[i].set_xlabel(r'$x_1$')\n",
    "\n",
    "    axs[i].set_ylabel(r'$x_2$')\n",
    "\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.show()"
   ],
   "id": "bed30c1596eb8519",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
