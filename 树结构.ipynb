{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "什么是决策树？决策树是一种利用树状数据结构来进行分类或者回归的算法。在决策树的生成中，我们通过信息论里面几个数值的计算判断分类例如熵，信息增益，增益率，基尼指数等，然后通过这些指标去生成一整个决策的树形图。\n",
    "信息论中的基本概念\n",
    "\n",
    "在决策树的生成中，我们通过信息论里面几个数值的计算判断分类：熵，信息增益，增益率，基尼指数。\n",
    "\n",
    "    信息熵：熵是衡量数据集中类别混乱程度的一个指标。在决策树中，我们使用熵来评估一个节点中的数据分布情况。如果一个节点中的数据大部分都属于同一个类别，那么这个节点的熵值就比较低，意味着这个节点的分类比较明确；反之，如果数据分布比较均匀，则熵值较高，表示这个节点的分类比较模糊。通过计算熵，我们可以确定在哪个节点进行分裂，以最大化分类的准确性。如果对于样本集合S，一共可以划分为k个类，每个类概率是pk，那么信息熵定义为：\n",
    "\n",
    "E(S)=−∑i=1kpklog⁡pk\n",
    "E(S)=−i=1∑k​pk​logpk​\n",
    "\n",
    "如果对数据集S按照某一个属性A对S进行划分，将它划分成v个子集，定义属性A的信息熵为：\n",
    "\n",
    "E(S,A)=∑i=1v∣Ai∣∣S∣E(Ai)\n",
    "E(S,A)=i=1∑v​∣S∣∣Ai​∣​E(Ai​)\n",
    "\n",
    "    信息增益：数据集本身的k个类是按照最后的返回标签排的，所以数据集本身的信息熵与属性的信息熵并不是一个东西。信息增益就是这个差值，看看按照这个属性划分我的信息到底增长了多少。信息增益用于衡量某个属性对于分类的贡献程度。在决策树中，我们选择信息增益最大的属性作为当前节点的分裂属性，这样可以使得子节点的数据更加集中，从而提高分类的准确性。\n",
    "\n",
    "Gain(A)=E(S)−E(S,A)\n",
    "Gain(A)=E(S)−E(S,A)\n",
    "\n",
    "    增益率：增益与信息熵的比值，它解决了信息增益偏向于选择具有多个值的属性问题。增益率结合了信息增益和属性熵，使得决策树在选择分裂属性时既考虑了分类的纯度，又考虑了属性的分散程度。\n",
    "\n",
    "GainRatio(A)=Gain(A)E(S,A)\n",
    "GainRatio(A)=E(S,A)Gain(A)​\n",
    "\n",
    "    基尼指数：基尼指数另一种衡量数据纯度的方式。它基于一个假设，即如果一个样本被错误分类，则该样本的基尼指数增加。因此，通过计算基尼指数，我们可以确定数据集的纯度，并选择能够最小化基尼指数的属性进行分裂。假设数据集有n个类，第k类的概率是pk，定义基尼指数：\n",
    "\n",
    "GINI(S)=1−∑i=1npk2\n",
    "GINI(S)=1−i=1∑n​pk2​\n",
    "\n",
    "通过引入这些信息论中的统计量，我们可以更好地构建决策树。它们帮助我们评估数据集的纯度，确定最佳的分裂属性，从而构建一棵结构简单、分类准确度高的决策树。这不仅提高了决策树的性能，也使得决策树在处理实际问题时更加可靠和有效。\n",
    "\n",
    "9.8.2 ID3决策树\n",
    "\n",
    "ID3决策树能够处理自变量和标签都是离散型的分类问题。它是以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。它是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法。\n",
    "\n",
    "ID3决策树的训练步骤分四步：\n",
    "\n",
    "（1）创建根节点，确定属性是什么。\n",
    "\n",
    "（2）若全部样本都是一类，那就全部落在叶子结点上。否则，根据每个属性计算信息增益，根据最大的信息增益确定划分属性与划分原则。\n",
    "\n",
    "（3）根据划分属性把属性值不同的样本划到对应边上。\n",
    "\n",
    "（4）根据不同属性的分类准则递归生成决策树。"
   ],
   "id": "cb7ee3df37dbb061"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "例如，用一个非常简单的案例实现ID3决策树。由于它只能处理离散自变量与离散标签的问题，这里选用的案例也比较简单，是一份贷款申请成功表，包含四个自变量：年龄段（青年、中年、老年），有工作（是、否），有自己的房子（是、否）和信贷情况（一般、好、非常好），最终标签是是否贷款成功。可以导入数据：",
   "id": "3aa301fae4546a3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:07:50.224810Z",
     "start_time": "2024-08-11T07:07:50.220796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "dataSet = [[0, 0, 0, 0, 'no'],         #数据集\n",
    "                      [0, 0, 0, 1, 'no'],\n",
    "                      [0, 1, 0, 1, 'yes'],\n",
    "                      [0, 1, 1, 0, 'yes'],\n",
    "                      [0, 0, 0, 0, 'no'],\n",
    "                      [1, 0, 0, 0, 'no'],\n",
    "                      [1, 0, 0, 1, 'no'],\n",
    "                      [1, 1, 1, 1, 'yes'],               \n",
    "                      [1, 0, 1, 2, 'yes'],\n",
    "                      [1, 0, 1, 2, 'yes'],\n",
    "                      [2, 0, 1, 2, 'yes'],\n",
    "                      [2, 0, 1, 1, 'yes'],\n",
    "                      [2, 1, 0, 1, 'yes'],\n",
    "                      [2, 1, 0, 2, 'yes'],\n",
    "                      [2, 0, 0, 0, 'no']]\n",
    "\n",
    "dataSet=np.array(dataSet)\n",
    "\n",
    "labels = ['年龄', '有工作', '有自己的房子', '信贷情况']        #分类属性"
   ],
   "id": "27a5c09680f26032",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "计算信息熵",
   "id": "d28a1357d484dded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:07:50.232232Z",
     "start_time": "2024-08-11T07:07:50.225812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def EntropyData(dataset):\n",
    "    n = len(dataset)                        #返回数据集的行数\n",
    "    dataset=dataset[:,-1]\n",
    "    count = np.unique(dataset, return_counts=True)[1]\n",
    "    ent = -np.sum([i/n * np.log2(i/n + 0.00001) for i in count]) # 防止出现log0\n",
    "    return ent"
   ],
   "id": "88c366a6ea36ba63",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "随后，如果选中了某个特征需要进行分类，则对其进行筛选和剔除操作。这个特征也被选中成为决策树的节点。",
   "id": "49a6c002fdd440a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:07:50.238678Z",
     "start_time": "2024-08-11T07:07:50.235239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def maxcount(y):\n",
    "    y,c = np.unique(y,return_counts=True)\n",
    "    return y[c==max(c)]\n",
    "def splitdata(dataset, f, value):\n",
    "    dataset = dataset[dataset[:, f] == value, :]       #使用布尔索引来选择 dataset 中满足条件 dataset[:, f] == value 的行。\n",
    "    # 这表示只保留那些在第 f 列上的值等于 value 的样本。\n",
    "    retDataSet=np.delete(dataset,f,1)\n",
    "    return retDataSet                                   #返回划分后的数据集"
   ],
   "id": "9e10091c378783fd",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:07:50.244736Z",
     "start_time": "2024-08-11T07:07:50.238678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def infoGain(fList, i,dataset):\n",
    "    baseEntropy = EntropyData(dataset)                 #计算数据集的香农熵\n",
    "    newEntropy = 0.0                                   #经验条件熵\n",
    "    for value in fList:                           #计算信息增益\n",
    "        subDataSet = splitdata(dataset, i, value)           #subDataSet划分后的子集\n",
    "        prob = len(subDataSet) / float(len(dataset))           #计算子集的概率\n",
    "        newEntropy += prob * EntropyData(subDataSet)        #根据公式计算经验条件熵\n",
    "    infoGain = baseEntropy - newEntropy                        #信息增益\n",
    "    return infoGain"
   ],
   "id": "5021ab916bb7420e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:07:50.264320Z",
     "start_time": "2024-08-11T07:07:50.256742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1                     #特征数量  \n",
    "    bestInfoGain = 0.0                                    #信息增益\n",
    "    bestFeature = -1                                      #最优特征的索引值\n",
    "    for i in range(numFeatures):                          #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataset]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        iGain = infoGain(uniqueVals,i,dataset)                        #信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, iGain))             #打印每个特征的信息增益\n",
    "        if (iGain > bestInfoGain):                              #计算信息增益\n",
    "            bestInfoGain = iGain                               #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                      #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                           #返回信息增益最大的特征的索引值\n",
    "def createID3(dataSet, labels, featLabels):\n",
    "\n",
    "    classList = [example[-1] for example in dataSet]              #取分类标签(是否放贷:yes or no)\n",
    "    if classList.count(classList[0]) == len(classList):        #如果类别完全相同则停止继续划分\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:                           #遍历完所有特征时返回出现次数最多的类标签\n",
    "        return maxcount(classList)\n",
    "    bestFeat = choose(dataSet)                   #选择最优特征\n",
    "    bestFeatLabel = labels[bestFeat]                               #最优特征的标签\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}                                    #根据最优特征的标签生成树\n",
    "    del(labels[bestFeat])                                          #删除已经使用特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]  #得到训练集中所有最优特征的属性值\n",
    "    uniqueVals = set(featValues)                                   #去掉重复的属性值\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:]\n",
    "        #递归调用函数createTree(),遍历特征，创建决策树。\n",
    "        myTree[bestFeatLabel][value] = createID3(splitdata(dataSet, bestFeat, value), subLabels, featLabels)\n",
    "    return myTree"
   ],
   "id": "80c52388013293c7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:07:50.273247Z",
     "start_time": "2024-08-11T07:07:50.265328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "featLabels = []\n",
    "myTree = createID3(dataSet, labels, featLabels)\n",
    "print(myTree)"
   ],
   "id": "b0ef0ef3b8c1d099",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "第0个特征的增益为0.252\n",
      "第1个特征的增益为0.918\n",
      "第2个特征的增益为0.474\n",
      "{'有自己的房子': {np.str_('0'): {'有工作': {np.str_('0'): np.str_('no'), np.str_('1'): np.str_('yes')}}, np.str_('1'): np.str_('yes')}}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "首先计算得到第0个特征的增益为0.083，第1个特征的增益为0.324，第2个特征的增益为0.420，第3个特征的增益为0.363。选择第2个特征分裂，删除它，还剩下三个特征。重新计算：第0个特征的增益为0.252，第1个特征的增益为0.918，第2个特征的增益为0.474。此时已经可以满足分类需求，递归停止。得到树的结构为：\n",
    "\n",
    "{'有自己的房子': {'1': 'yes', '0': {'有工作': {'1': 'yes', '0': 'no'}}}}"
   ],
   "id": "5798962513ad3ff0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "26324b196b503948"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "    根节点：\n",
    "        决策树的根节点是 '有自己的房子'，这意味着在进行分类时，首先考虑这个特征。\n",
    "\n",
    "    子节点：\n",
    "        对于 '有自己的房子' 这个特征，可能的值是 0 或 1。\n",
    "            如果 '有自己的房子' 的值为 0，则进一步考虑 '有工作' 这个特征。\n",
    "                如果 '有工作' 的值为 0，则分类结果为 no。\n",
    "                如果 '有工作' 的值为 1，则分类结果为 yes。\n",
    "            如果 '有自己的房子' 的值为 1，则分类结果直接为 yes，不再考虑其他特征。\n"
   ],
   "id": "6fcdedf493946d9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了处理连续属性的自变量和缺失值问题，对ID3决策树进行改进就得到了C4.5决策树，后来又诞生了C5.0决策树。C4.5决策树在每个节点处都会选择一个最佳的属性进行分支，选择的标准通常是信息增益或增益率。信息增益代表信息不确定性较少的程度，信息增益越大，说明不确定性降低的越多，因此该特征对分类来说越重要。C4.5通过阈值自动把连续变量分成两部分来处理，在数据特性和基本结构上具有很强的灵活性和泛化能力，能够处理各种类型的数据，并构建出准确度高的分类模型。\n",
    "\n",
    "C4.5决策树的本质是一种分治策略，基本步骤包括：\n",
    "\n",
    "（1）将连续数值离散化，创建树。\n",
    "\n",
    "（2）确定连续属性的阈值，计算信息增益率，确定划分属性。\n",
    "\n",
    "（3）根据划分属性把属性值不同的样本划到对应边上。\n",
    "\n",
    "（4）根据不同属性的分类准则递归生成树。\n",
    "\n",
    "有了前面ID3的基础，实现C4.5其实非常简单。首先需要定义信息增益率的写"
   ],
   "id": "f11bbda7891a70c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def splitdata_C4_5(dataset, f, value):\n",
    "\n",
    "    dataset = dataset[dataset[:, f] <= value, :]       \n",
    "\n",
    "    retDataSet=np.delete(dataset,f,1)\n",
    "\n",
    "    return retDataSet                                   #返回划分后的数据集\n",
    "\n",
    "def infoGain_rate(fList,i,dataset):\n",
    "\n",
    "    H = EntropyData(dataset)\n",
    "\n",
    "    IG = infoGain(fList,i,dataset)\n",
    "\n",
    "    return IG/H"
   ],
   "id": "35e3fcd9a153868"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "然后使用这两处替换ID3代码的对应主体部分即可。得到的生成树结构为：\n",
    "\n",
    "{'信贷情况': {'no': {'有自己的房子': {'1': '0', '0': '0'}}, 'yes': {'有自己的房子': {'1': {'有工作': {'1': {'年龄': {'1': array(['0'], dtype='<U1'), '0': array(['0'], dtype='<U1')}}, '0': {'年龄': {'1': array(['0'], dtype='<U1'), '0': array(['0'], dtype='<U1')}}}}, '2': {'有工作': {'1': {'年龄': {'1': array(['0', '1', '2'], dtype='<U1'), '0': array(['1'], dtype='<U1')}}, '0': {'年龄': {'1': array(['0'], dtype='<U1'), '0': array(['0'], dtype='<U1')}}}}, '0': {'有工作': {'1': {'年龄': {'1': array(['0'], dtype='<U1'), '0': array(['0'], dtype='<U1')}}, '0': '0'}}}}}}"
   ],
   "id": "5e923a4857ec9922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ART（Classification and Regression Trees）决策树算法在先前的工作基础上进行了一些重要的改进，使其在处理各种数据和问题时更加有效。顾名思义，CART决策树算法是能够处理回归问题的一种树算法。它适用于各种类型的数据和问题，尤其是处理大规模数据集和多变量输入的情况。CART算法适用于各种类型的数据和问题，包括分类和回归问题，以及连续属性和缺失值的处理，并且有很好的可解释性。Python的sklearn中集成的默认树模型是基于CART结构的。\n",
    "\n",
    "CART决策树处理分类问题的步骤包括：\n",
    "\n",
    "（1）将连续数值离散化，创建树。\n",
    "\n",
    "（2）确定连续属性的阈值，计算基尼指数增长，确定划分属性，按最小者开始划分，注意要二分。\n",
    "\n",
    "（3）根据划分属性把属性值不同的样本划到对应边上。\n",
    "\n",
    "（4）根据不同属性递归计算基尼指数增长并划分。\n",
    "\n",
    "如果CART决策树处理的的是一个回归问题，则回归中的基尼指数为\n",
    "\n",
    "GainGINI=yk1−μ1+yk2−μ2\n",
    "GainGINI=yk1​−μ1​\n",
    "​+yk2​−μ2​\n",
    "\n",
    "​\n",
    "\n",
    "CART使用基尼指数替换即可，这里不多对上述代码进行改写。这里注意在决策树训练过程中有一个重要的技巧：剪枝。决策树生成中需要剪枝的原因主要是为了防止过拟合。通过剪枝，可以去除决策树中过于复杂的部分，使其在测试数据或实际应用中表现更好。剪枝包括预剪枝和后剪枝两种方法。预剪枝是指在决策树生成过程中提前停止树的生长，以防止过拟合，可以及早停止树的生长，从而减少过拟合的风险。然而，预剪枝可能会过早地停止树的生长，导致丢失一些重要的特征和样本。后剪枝则是在决策树生成完成后对其进行简化，以去除不必要或冗余的节点，可以保留更多的特征和样本，但可能会导致过拟合。如果数据集较小，可以采用预剪枝方法，以减少过拟合的风险。如果数据集较大，特征不重要或者存在很多噪声，可以采用后剪枝方法，以保留更多的特征和样本。另外，也可以结合使用预剪枝和后剪枝，先进行预剪枝再进行后剪枝，以获得更好的剪枝效果。\n",
    "\n",
    "在使用CART（分类与回归树）算法构建决策树时，我们通常会依赖于基尼指数来选择最佳的分裂点。基尼指数是一个衡量数据不纯度的指标，分裂点的选择目标是最大化基尼指数的降低，从而得到更纯净的子节点。尽管基尼指数是一个有效的分裂准则，但在决策树的训练过程中，还需要考虑另一个重要的技术——剪枝。\n",
    "\n",
    "剪枝的目的是为了防止决策树过拟合。过拟合是指模型在训练数据上表现很好，但在新的、未见过的数据上表现不佳的现象。过拟合的决策树通常具有过多的节点，这些节点可能会捕捉到数据中的噪声和偶然规律，而不是真实的、普遍存在的模式。\n",
    "\n",
    "剪枝有两种主要策略：预剪枝和后剪枝。\n",
    "\n",
    "1. 预剪枝：\n",
    "\n",
    "预剪枝是在决策树生长的过程中进行的。它通过设置一些停止条件（例如，树的最大深度、节点的最小样本数、节点的最小信息增益等）来限制树的生长。这种方法的优点是可以减少过拟合的风险，因为它避免了树变得过于复杂。然而，预剪枝也有可能过早地停止树的生长，这可能导致模型未能充分捕捉数据中的所有重要信息，从而影响模型的准确性。\n",
    "\n",
    "2. 后剪枝：\n",
    "\n",
    "后剪枝则是在决策树完全生长后进行的。它从树的底部开始，检查每个非叶子节点，如果移除该节点并用其子节点替换可以减少模型的复杂度，同时保持或提高预测性能，那么就进行剪枝。后剪枝的优点是它允许树在生成过程中充分生长，从而保留更多的特征和样本信息。但缺点是如果剪枝不当，仍有可能导致过拟合。\n",
    "\n",
    "在实际应用中，可以根据数据集的特点和需求来选择剪枝策略。例如，如果数据集较小，或者特征较为重要，可以采用预剪枝来减少过拟合的风险。如果数据集较大，或者特征中包含较多的噪声，可以采用后剪枝来保留更多的信息。此外，还可以结合预剪枝和后剪枝的优点，先进行预剪枝以控制树的生长，再进行后剪枝以进一步优化模型。\n",
    "\n",
    "通过适当的剪枝，我们可以创建出既能捕捉数据中的关键信息，又具有良好的泛化能力的决策树模型。在实践中，剪枝是一个需要细致调整的过程，可能需要多次尝试和评估，才能找到最佳的剪枝策略。\n",
    "\n",
    "在剪枝后评估剪枝后决策树模型的性能是一个关键的过程，它帮助我们理解模型在处理未知数据时的有效性。以下是详细的评估步骤：\n",
    "\n",
    "1. 交叉验证：\n",
    "\n",
    "交叉验证通过将数据集分割成多个子集，并在这些子集上重复训练和测试模型，来评估模型的稳定性和可靠性。在k折交叉验证中，每个子集轮流作为测试集，其余的作为训练集。这样可以确保模型在不同数据子集上的表现都得到评估，从而提高模型选择的信心。\n",
    "\n",
    "2.性能指标：\n",
    "\n",
    "根据问题的性质选择合适的性能指标。例如，在分类问题中，除了准确率，我们还关注精确率和召回率，它们分别衡量了模型正确识别正例的能力。在回归问题中，我们更关注预测值与实际值之间的差异，因此会使用MSE、RMSE或MAE等指标。\n",
    "\n",
    "3. 模型比较：\n",
    "\n",
    "如果有多个剪枝策略生成的决策树模型，我们需要比较它们的性能指标，以选择最佳的模型。这通常涉及到在模型的复杂度和预测性能之间找到平衡点。一个简单但过于复杂的模型可能会过拟合，而一个复杂度适中的模型可能更具有泛化能力。\n",
    "\n",
    "4. 学习曲线：\n",
    "\n",
    "学习曲线显示了模型性能随数据量或训练时间的增加而变化的趋势。通过分析学习曲线，我们可以识别出模型是否开始过拟合。如果模型在训练集上的性能持续提高，而在验证集上的性能开始下降，这通常表明模型开始捕捉到训练数据中的噪声。\n",
    "\n",
    "5. 外部验证：\n",
    "\n",
    "在完成所有模型调整和训练后，使用一个从未参与过训练的独立测试集来评估模型的最终性能。这个步骤是至关重要的，因为它提供了模型在实际应用中可能表现的真实估计。通过外部验证，我们可以确信模型的泛化能力，以及它对新数据的适应性。\n",
    "\n",
    "掌握这些评估技术对于构建和选择适当的决策树模型至关重要。它们不仅帮助我们避免过拟合或欠拟合的问题，还能确保我们的模型在现实世界中的表现与在训练集上的表现一样好。通过综合运用这些方法，我们可以提高模型的可靠性和实用性。"
   ],
   "id": "de2a1986f898774a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
